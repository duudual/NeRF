## Transformer-based NeRF (T-nerf)
In this part, we construct a feedforward network for NERF,based on vggt.
To make use of the pre-trained weights, we must keep the basic achitecture unchanged.

- #### T-nerf
- Plan A:
    predict a two-layer MLP as the nerf-MLP.
- Plan B: (reject)
    simply based on point cloud -- construct the ncolorHead with six more output dims to predict 6 colors(up down left right front behind),which worked as a colored point map for NERF.
    using both pointHead and ncolorHead for nerf.

- #### Used for Dynamic Nerf
get features generated by encoder from two frames, then interpolate them to get a fused feature representing the specified frame during these frames.Then we can use the decoder to get the spcified scene.

- #### Training
- NLPHead: 利用输出的MLP执行一次NERF的执行与损失计算.很像风格迁移
- ncolorhead: detailed in Data Generation.

- #### Data Generation
传入数据是多张图像，在每个图像上各自执行一次3D点的预测，预测出一系列的点，得到一个串，所有的串stack在一起，得到多个长串，最后再相应得加上一些串，这些点是随机取的点，这些点不在点云上，按理来说这些点应该是空气中的点，这些点的透明度都是1，然后颜色都是白色，以求让模型能够学到空气的点应该忽略。然后，预留一个NeRF的MLP执行的函数接口，这个函数还没实现，你可以先定义着使用。传入这个点的串，获得这些点获得上下左右前后6个点的函数值以及不透明度。这样，由于与传入数据一一对应，所以我们就获得了，每个2D图像点对应的那个3D点，他的colormap的数值。

- #### mile stone:
    - 问题描述：虽然显示出很好的渲染性能，但NERF并不是一个通用的渲染器——它对每个场景都需要执行一次训练过程，而不是构建一个通用的渲染器实现对所有场景的快速渲染。因此，我们希望构建一个通用的渲染器，传入多视角图像，不需要进行额外的针对该场景的训练，得到一个对该场景的渲染。使用的数据来自NERF的官方数据集。
    - Method-1: 由于算力的限制，我们没法从头训练一个具备通过2D图像感知3D空间的模型，因此我们将2025 cypr best-paper(VGGT:https://arxiv.org/abs/2503.11651)视为预训练模型，利用其全局空间感知能力，在其backbone的基础上加入我们编写的head,预测一个MLP参数以进行NERF的渲染。损失计算与nerf一致，冻结vggt参数，优化head。evaluation方式为量化的评估原视角图像的还原程度，以及与传统nerf效果的对比. 我从未见过一个模型去预测mlp参数……但我想以此为契机进行尝试。


- #### thought:
有个很有意思的想法是，我刚刚思考到什么能够代表一个3D场景时，突然想到用一个mlp表示一个3D场景很不错（这里的代表是，图片是表示一个2D，这里mlp的地位与图片的地位类似，而不是和高维向量的地位类似，但这个点上，需要mlp尽可能的是一个唯一的双射的那种，这里如果可以通过之前的proResnet来尽可能减少无关的参数和连接，那这个就更符合我们想要的性质了），可以通过投影到平面上，获得2D视角图片，这个可以满足作为场景表示，应用到机器人上时，可以投影到其视角到一个看到的图像，正常接入后面的视觉系统，而且在训练时，也能够利用此进行loss计算与优化。

其作为一个场景表示的话，一个backwards就是没法精细调控里面的内容。这里还有很多能思考的空间，比如vcl提到的几张图片生成一个连续场景的技术，比如如何将3D建模软件的结果平滑转接过去

对于这个项目来说，尝试使用vggt来输出一个mlp是一个很好的尝试。这个项目的好处是，1 这是正向获得mlp的方法，传统nerf是逆向，传统的这个缺陷是阻碍mlp表示方法推广的重要原因；2 这种正向的方法，前面语义特征，这是很好的，比如匹配了，语义理解了，都有好处。这种正向的方法可以成为生成mlp的重要方法

26-1-5:
天无绝人之路！本来觉得要寄了，突然想到昨天偶然查到的mlp-mae已经开源的那个不就是辐射场吗！我直接对里面的点进行mlp预测，还省下了投影的事情！

- 现在我要做的事情是，使用nerf-mae中的pretrain的数据，为我的项目提供数据。pretrain的数据我已经下载到目录pretrain当中，数据内容是场景的网格，存储的有每个点的rgb与sigma。
我的项目是在NeRF中的tnerf目录下，主要思路是利用vggt，实现一个根据多视角图像直接预测出nerf使用的那个mlp的参数。训练是，基于nerf-mae提供的场景数据，首先渲染出一些对这个场景的多视角图像，然后将这些图像传入vggt再通过设定的nlphead输出mlp，接着在这个场景中，利用此mlp计算各个点上的预测取值，计算损失。目前的项目代码中一部分是拿来了vggt的相关代码，不全是与项目相关。

所以，你需要做的可以归结为下面几点
1.实现一个数据生成pipeline.参考nerf-mae中现有的数据读取与处理方法，将nerf-mae的预训练数据读取出来，然后渲染出一些视角的图像。
2.写训练的程序。这个你可以参考现有的训练是如何写的。

26-1-11
Hybrid Grid + Tiny MLP (混合网格表示)
模仿 Instant-NGP 的思路，但将网格变为“预测值”。
思路：
特征网格 (Feature Grid)： 用你的 VGGT 预测一个低分辨率的 3D 稠密网格（或哈希表）。
小 MLP： 后面接一个非常浅（例如只有 2 层，宽度 64）的 MLP。
核心逻辑： 场景的复杂几何信息存储在网格里，而 MLP 只负责学习光影解码。你只需要让超网络预测那个网格，而 MLP 可以是预训练好或全局共享的。
下面我将系统性的阐述新的实现的方法：
1.nlphead修改为latentHead，仍然大体上与其他head一样，以dpthead为基础的类型，这个head会预测出Tri-plane，三个64x64的特征网格，代表xy xz yz平面的信息，每个位置有32维的向量。然后还有一个全局共享的MLP层。
2.一次渲染的逻辑：首先将多张照片输入到vggt中，然后获得latentHead的输出，对于一次对某个位置[x,y,z和方向]的查询，
(1)首先根据x y z 位置，分别利用xy坐标在xy平面中，进行双线性插值，得到一个32维的特征向量，另外两个平面同理，得到3个特征向量，然后concat，
(2)对位置进行编码，然后加入朝向，从而有三种输入，然后输出rgb sigma
3.损失里，要加入正则化项，加入latentHead输出的网格的特征向量的正则化

26-1-12
好了，下面我将写一个全新的dynamic nerf的实现方案,这是一种新的插值实现手段，DynamicVGGT类通过一个参数选择不同的插值方案。
1.我的输入时，两个时刻的照片组，我已经确保，两组照片之间顺序上完全一一对应，相同顺序的照片具有完全一样的相机内外参
2.这种新的插值发生在vggt对patch分割然后使用dinov2获得特征向量之后。对于t0时刻的一张照片的一个patch，在t1时刻该Patch的邻域中，利用余弦相似度找到最相符的那个patch，两个特征向量线性插值，计算t0到t1之间任意时刻的2d位置与特征，根据这个2d位置，利用双线性泼溅的方法，找到该2d位置最临近的四个patch，将特征按权重分配给这四个网格点上，在执行双线性泼溅时，需要额外维护一个权重网格 (Weight Grid)。最终输出的插值 Token 需要除以该权重网格进行归一化，如果有些patch的权重是0，那么在时间的前半段用t0对应的patch的填充，后半段用t1的对应patch填充。在处理所有的patch后，就得到了在ti时刻，插值后的这张照片的token.
3.然后正常的输入到注意力层以及后续的head层。
4.在实现上，你要额外实现一个函数，这个函数是获得一个列表，包含t0~t1这一段时间的采样的输出结果的列表。在实现中，为了优化，你可以将t0和t1的patch的输出缓存，然后在求中间时刻时读取计算。