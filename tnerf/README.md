## Transformer-based NeRF (T-nerf)
In this part, we construct a feedforward network for NERF,based on vggt.
To make use of the pre-trained weights, we must keep the basic achitecture unchanged.

- #### T-nerf
- Plan A:
    predict a two-layer MLP as the nerf-MLP.
- Plan B: (reject)
    simply based on point cloud -- construct the ncolorHead with six more output dims to predict 6 colors(up down left right front behind),which worked as a colored point map for NERF.
    using both pointHead and ncolorHead for nerf.

- #### Used for Dynamic Nerf
get features generated by encoder from two frames, then interpolate them to get a fused feature representing the specified frame during these frames.Then we can use the decoder to get the spcified scene.

- #### Training
- NLPHead: 利用输出的MLP执行一次NERF的执行与损失计算.很像风格迁移
- ncolorhead: detailed in Data Generation.

- #### Data Generation
传入数据是多张图像，在每个图像上各自执行一次3D点的预测，预测出一系列的点，得到一个串，所有的串stack在一起，得到多个长串，最后再相应得加上一些串，这些点是随机取的点，这些点不在点云上，按理来说这些点应该是空气中的点，这些点的透明度都是1，然后颜色都是白色，以求让模型能够学到空气的点应该忽略。然后，预留一个NeRF的MLP执行的函数接口，这个函数还没实现，你可以先定义着使用。传入这个点的串，获得这些点获得上下左右前后6个点的函数值以及不透明度。这样，由于与传入数据一一对应，所以我们就获得了，每个2D图像点对应的那个3D点，他的colormap的数值。

- #### mile stone:
    - 问题描述：虽然显示出很好的渲染性能，但NERF并不是一个通用的渲染器——它对每个场景都需要执行一次训练过程，而不是构建一个通用的渲染器实现对所有场景的快速渲染。因此，我们希望构建一个通用的渲染器，传入多视角图像，不需要进行额外的针对该场景的训练，得到一个对该场景的渲染。使用的数据来自NERF的官方数据集。
    - Method-1: 由于算力的限制，我们没法从头训练一个具备通过2D图像感知3D空间的模型，因此我们将2025 cypr best-paper(VGGT:https://arxiv.org/abs/2503.11651)视为预训练模型，利用其全局空间感知能力，在其backbone的基础上加入我们编写的head,预测一个MLP参数以进行NERF的渲染。损失计算与nerf一致，冻结vggt参数，优化head。evaluation方式为量化的评估原视角图像的还原程度，以及与传统nerf效果的对比. 我从未见过一个模型去预测mlp参数……但我想以此为契机进行尝试。