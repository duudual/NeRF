# Copyright (c) Meta Platforms, Inc. and affiliates.
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.

"""
T-NeRF Data Loaders for NColorHead and NLPHead training.

NColorDataset:
- Loads preprocessed NColor training data
- Each sample contains: 2D image points -> 3D point colormap values

NLPDataset:
- Loads multi-view image data for NLP training
- Each sample contains: multi-view images for NeRF MLP parameter prediction
"""

import os
import json
import numpy as np
from typing import Dict, Any, Optional, List, Tuple
from pathlib import Path

import torch
from torch.utils.data import Dataset, DataLoader
from PIL import Image
import torchvision.transforms as T


class NColorDataset(Dataset):
    """
    Dataset for NColor (color map) training.
    
    Each sample contains:
    - images: Multi-view images [S, 3, H, W]
    - target_colors: Target color map values [S, H, W, 18] (6 directions * 3 RGB)
    - target_alpha: Target alpha values [S, H, W, 1]
    - point_masks: Valid point masks [S, H, W]
    - is_air_point: Air point masks [S, H, W]
    
    Data is generated by ncolor_data_generator.py
    """
    
    def __init__(
        self,
        data_dir: str,
        split: str = "train",
        image_size: Tuple[int, int] = (518, 518),
        num_views: int = 8,
        transform: Optional[T.Compose] = None,
    ):
        """
        Initialize NColor dataset.
        
        Args:
            data_dir: Directory containing NColor training data
            split: "train" or "val"
            image_size: Target image size (H, W)
            num_views: Number of views per sample
            transform: Optional image transforms
        """
        self.data_dir = Path(data_dir)
        self.split = split
        self.image_size = image_size
        self.num_views = num_views
        
        # Default transform
        if transform is None:
            self.transform = T.Compose([
                T.Resize(image_size),
                T.ToTensor(),
            ])
        else:
            self.transform = transform
        
        # Load data index
        self.samples = self._load_index()
    
    def _load_index(self) -> List[Dict]:
        """Load dataset index from data directory."""
        index_path = self.data_dir / f"{self.split}_index.json"
        
        if index_path.exists():
            with open(index_path, "r") as f:
                return json.load(f)
        else:
            # Fallback: scan directory for samples
            samples = []
            sample_dirs = sorted(self.data_dir.glob(f"{self.split}_*"))
            for sample_dir in sample_dirs:
                if sample_dir.is_dir():
                    samples.append({"path": str(sample_dir)})
            return samples
    
    def __len__(self) -> int:
        return len(self.samples)
    
    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        sample_info = self.samples[idx]
        sample_path = Path(sample_info["path"])
        
        # Load images
        images = []
        for i in range(self.num_views):
            img_path = sample_path / f"image_{i:04d}.png"
            if img_path.exists():
                img = Image.open(img_path).convert("RGB")
                img = self.transform(img)
                images.append(img)
        
        if len(images) < self.num_views:
            # Pad with zeros if not enough images
            for _ in range(self.num_views - len(images)):
                images.append(torch.zeros(3, *self.image_size))
        
        images = torch.stack(images, dim=0)  # [S, 3, H, W]
        
        # Load target colors
        colors_path = sample_path / "target_colors.npy"
        if colors_path.exists():
            target_colors = torch.from_numpy(np.load(colors_path)).float()
        else:
            target_colors = torch.zeros(self.num_views, *self.image_size, 18)
        
        # Load target alpha
        alpha_path = sample_path / "target_alpha.npy"
        if alpha_path.exists():
            target_alpha = torch.from_numpy(np.load(alpha_path)).float()
        else:
            target_alpha = torch.zeros(self.num_views, *self.image_size, 1)
        
        # Load point masks
        mask_path = sample_path / "point_masks.npy"
        if mask_path.exists():
            point_masks = torch.from_numpy(np.load(mask_path)).float()
        else:
            point_masks = torch.ones(self.num_views, *self.image_size)
        
        # Load air point masks
        air_path = sample_path / "is_air_point.npy"
        if air_path.exists():
            is_air_point = torch.from_numpy(np.load(air_path)).float()
        else:
            is_air_point = torch.zeros(self.num_views, *self.image_size)
        
        return {
            "images": images,
            "target_colors": target_colors,
            "target_alpha": target_alpha,
            "point_masks": point_masks,
            "is_air_point": is_air_point,
        }


class NLPDataset(Dataset):
    """
    Dataset for NLP (NeRF MLP Parameters) training.
    
    Each sample contains:
    - images: Multi-view images [S, 3, H, W]
    - target_render: Target rendered images for supervision (optional)
    - ray_origins: Ray origins for volume rendering (optional)
    - ray_directions: Ray directions for volume rendering (optional)
    
    This dataset loads multi-view images for NLP training.
    The loss is computed by rendering with predicted MLP parameters
    and comparing with the input images.
    """
    
    def __init__(
        self,
        data_dir: str,
        split: str = "train",
        image_size: Tuple[int, int] = (518, 518),
        num_views: int = 8,
        transform: Optional[T.Compose] = None,
    ):
        """
        Initialize NLP dataset.
        
        Args:
            data_dir: Directory containing multi-view image data
            split: "train" or "val"
            image_size: Target image size (H, W)
            num_views: Number of views per sample
            transform: Optional image transforms
        """
        self.data_dir = Path(data_dir)
        self.split = split
        self.image_size = image_size
        self.num_views = num_views
        
        if transform is None:
            self.transform = T.Compose([
                T.Resize(image_size),
                T.ToTensor(),
            ])
        else:
            self.transform = transform
        
        self.samples = self._load_index()
    
    def _load_index(self) -> List[Dict]:
        """Load dataset index."""
        index_path = self.data_dir / f"{self.split}_scenes.json"
        
        if index_path.exists():
            with open(index_path, "r") as f:
                return json.load(f)
        else:
            # Scan for scene directories
            samples = []
            scene_dirs = sorted(self.data_dir.glob("scene_*"))
            for scene_dir in scene_dirs:
                if scene_dir.is_dir():
                    samples.append({"path": str(scene_dir)})
            return samples
    
    def __len__(self) -> int:
        return len(self.samples)
    
    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        sample_info = self.samples[idx]
        sample_path = Path(sample_info["path"])
        
        # Load images
        images = []
        image_files = sorted(sample_path.glob("*.png")) + sorted(sample_path.glob("*.jpg"))
        
        for i, img_path in enumerate(image_files[:self.num_views]):
            img = Image.open(img_path).convert("RGB")
            img = self.transform(img)
            images.append(img)
        
        # Pad if necessary
        while len(images) < self.num_views:
            images.append(images[-1] if images else torch.zeros(3, *self.image_size))
        
        images = torch.stack(images[:self.num_views], dim=0)  # [S, 3, H, W]
        
        result = {"images": images}
        
        # Load camera parameters if available
        camera_path = sample_path / "cameras.npz"
        if camera_path.exists():
            camera_data = np.load(camera_path)
            if "extrinsics" in camera_data:
                result["camera_extrinsics"] = torch.from_numpy(camera_data["extrinsics"]).float()
            if "intrinsics" in camera_data:
                result["camera_intrinsics"] = torch.from_numpy(camera_data["intrinsics"]).float()
        
        return result


class CombinedDataset(Dataset):
    """
    Combined dataset for training both NColorHead and NLPHead.
    
    Wraps both NColorDataset and NLPDataset to provide samples
    for combined training.
    """
    
    def __init__(
        self,
        nlp_data_dir: str,
        ncolor_data_dir: Optional[str] = None,
        split: str = "train",
        image_size: Tuple[int, int] = (518, 518),
        num_views: int = 8,
    ):
        """
        Initialize combined dataset.
        
        Args:
            nlp_data_dir: Directory for NLP training data (multi-view images)
            ncolor_data_dir: Directory for NColor training data (optional, uses nlp_data_dir if None)
            split: "train" or "val"
            image_size: Target image size
            num_views: Number of views per sample
        """
        self.nlp_dataset = NLPDataset(
            data_dir=nlp_data_dir,
            split=split,
            image_size=image_size,
            num_views=num_views,
        )
        
        ncolor_dir = ncolor_data_dir if ncolor_data_dir else nlp_data_dir
        self.ncolor_dataset = NColorDataset(
            data_dir=ncolor_dir,
            split=split,
            image_size=image_size,
            num_views=num_views,
        )
        
        # Use the larger dataset length
        self.length = max(len(self.nlp_dataset), len(self.ncolor_dataset))
    
    def __len__(self) -> int:
        return self.length
    
    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        # Get NLP sample (cycle if needed)
        nlp_idx = idx % len(self.nlp_dataset)
        nlp_sample = self.nlp_dataset[nlp_idx]
        
        # Get NColor sample (cycle if needed)
        ncolor_idx = idx % len(self.ncolor_dataset)
        ncolor_sample = self.ncolor_dataset[ncolor_idx]
        
        # Combine samples (NColor data takes precedence for shared keys)
        combined = {**nlp_sample, **ncolor_sample}
        
        return combined


def create_dataloaders(
    mode: str,
    data_dir: str,
    ncolor_data_dir: Optional[str] = None,
    batch_size: int = 2,
    num_workers: int = 4,
    image_size: Tuple[int, int] = (518, 518),
    num_views: int = 8,
) -> Tuple[DataLoader, DataLoader]:
    """
    Create training and validation dataloaders.
    
    Args:
        mode: "ncolor", "nlp", or "both"
        data_dir: Main data directory
        ncolor_data_dir: Optional separate directory for NColor data
        batch_size: Batch size
        num_workers: Number of data loading workers
        image_size: Target image size
        num_views: Number of views per sample
    
    Returns:
        Tuple of (train_loader, val_loader)
    """
    if mode == "ncolor":
        train_dataset = NColorDataset(
            data_dir=ncolor_data_dir or data_dir,
            split="train",
            image_size=image_size,
            num_views=num_views,
        )
        val_dataset = NColorDataset(
            data_dir=ncolor_data_dir or data_dir,
            split="val",
            image_size=image_size,
            num_views=num_views,
        )
    elif mode == "nlp":
        train_dataset = NLPDataset(
            data_dir=data_dir,
            split="train",
            image_size=image_size,
            num_views=num_views,
        )
        val_dataset = NLPDataset(
            data_dir=data_dir,
            split="val",
            image_size=image_size,
            num_views=num_views,
        )
    else:  # mode == "both"
        train_dataset = CombinedDataset(
            nlp_data_dir=data_dir,
            ncolor_data_dir=ncolor_data_dir,
            split="train",
            image_size=image_size,
            num_views=num_views,
        )
        val_dataset = CombinedDataset(
            nlp_data_dir=data_dir,
            ncolor_data_dir=ncolor_data_dir,
            split="val",
            image_size=image_size,
            num_views=num_views,
        )
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True,
        drop_last=True,
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True,
    )
    
    return train_loader, val_loader
